#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Test avec debug pour detect_bipartite_cross_tokens."""

import sys
import os
import re
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from prompts.runner import tokenize_with_offsets, normalize_spaces

# Copie de la fonction avec debug
def debug_detect_bipartite_cross_tokens(text, tokens, existing_cues, rule):
    out = []
    max_tokens = rule.get("options", {}).get("max_token_gap")
    if not max_tokens:
        print("‚ùå Pas de max_token_gap d√©fini")
        return out
    try:
        max_tokens = int(max_tokens)
    except Exception:
        max_tokens = 8
    
    print(f"‚úÖ max_tokens: {max_tokens}")
    
    part1_set = {"ne", "n'", "n'"}
    part2_set = {"pas", "plus", "jamais", "rien", "personne", "gu√®re", "point", "nul"}
    existing_keys = {(c.get("id"), c.get("start"), c.get("end")) for c in existing_cues}
    
    print(f"‚úÖ part1_set: {part1_set}")
    print(f"‚úÖ part2_set: {part2_set}")
    
    for i, (tok, a, b) in enumerate(tokens):
        t = tok.lower()
        print(f"\n--- Token {i}: '{tok}' ('{t}') ---")
        
        # un marqueur de d√©but peut √™tre "ne", ou bien un token d√©butant par "n'" ou "n'"
        # G√©rer aussi les contractions s√©par√©es par le tokeniseur: "n" + "'"
        is_part1 = t == "ne" or t.startswith("n'") or t.startswith("n'")
        print(f"is_part1 initial: {is_part1}")
        
        # G√©rer le cas o√π "n'" est tokenis√© en "n" + "'"
        contraction_start = a
        contraction_token = tok
        if t == "n" and i + 1 < len(tokens) and tokens[i + 1][0] in ["'", "'"]:
            is_part1 = True
            contraction_token = "n'"  # Reconstruire la contraction
            contraction_start = a
            print(f"Contraction d√©tect√©e: '{contraction_token}'")
            
        print(f"is_part1 final: {is_part1}")
        
        if is_part1:
            print(f"üîç Recherche de part2 √† partir du token {i+1}")
            gap = 0
            start_search = i + 2 if (t == "n" and i + 1 < len(tokens) and tokens[i + 1][0] in ["'", "'"]) else i + 1
            print(f"start_search: {start_search}")
            
            for j in range(start_search, len(tokens)):
                t2, a2, b2 = tokens[j]
                print(f"  Token {j}: '{t2}' -> gap {gap}")
                
                # ignore pure punctuation tokens
                PUNCT_RE = r"[,;:!?\.()\[\]{}¬´¬ª""\"']"
                if re.match(PUNCT_RE, t2):
                    print(f"    Ponctuation ignor√©e")
                    continue
                gap += 1
                if gap > max_tokens:
                    print(f"    Gap trop grand ({gap} > {max_tokens})")
                    break
                    
                if t2.lower() in part2_set:
                    print(f"  ‚úÖ Part2 trouv√©: '{t2}'")
                    key = (rule.get("id"), contraction_start, b2)
                    if key not in existing_keys:
                        label = normalize_spaces(contraction_token + " " + t2)
                        print(f"  ‚úÖ Cue cr√©√©: '{label}'")
                        
                        out.append({
                            "id": rule.get("id", "NE_BIPARTITE_EXTENDED"),
                            "cue_label": label,
                            "start": contraction_start,
                            "end": b2,
                            "group": rule.get("group", "bipartite")
                        })
                    else:
                        print(f"  ‚ö†Ô∏è Cue d√©j√† existant")
                    break
                else:
                    print(f"    '{t2}' pas dans part2_set")
    
    return out

def test_debug():
    text = "Les patients n'ont pas pr√©sent√© de complication postop√©ratoire."
    tokens = tokenize_with_offsets(text)
    
    print(f"Texte: {text}")
    
    rule = {
        "id": "NE_BIPARTITE_EXTENDED",
        "group": "bipartite",
        "options": {
            "max_token_gap": 8,
            "exclude_verbs_from_cue": True
        }
    }
    
    cues = debug_detect_bipartite_cross_tokens(text, tokens, [], rule)
    
    print(f"\n=== R√âSULTAT ===")
    print(f"Cues d√©tect√©s: {len(cues)}")
    for cue in cues:
        print(f"  Cue: '{cue['cue_label']}' (start: {cue['start']}, end: {cue['end']})")

if __name__ == "__main__":
    test_debug()
