#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Runner permissif v3 â€” 100% sans spaCy (fullâ€‘regex + tokenisation maison)

Ce build aligne la sortie sur le pipeline que tu demandes :
- Ã‰tape 1 (dÃ©terministe) : cues + scopes via rÃ¨gles YAML et stratÃ©gies surface.
- Ã‰tape 2 (LLM actions) : dictÃ© par un contrat OBLIGATOIRE.
- Ã‰tape 3 (LLM ajouts) : champ "LLM_ajouts" rÃ©servÃ© pour les complÃ©ments (sujet, supports, cues manquants),
  Ã  remplir cÃ´tÃ© orchestrateur.

Diffs clÃ©s vs version prÃ©cÃ©dente :
- Ajout d'un coupeur lexical (stop_lexemes) pour couper la portÃ©e bipartite avant les concessives (ex. Â« malgrÃ© Â», Â« mais Â»).
- PrÃ©-dÃ©tection dÃ©terministe de certaines prÃ©positions concessives frÃ©quentes (ex. Â« malgrÃ© Â») si absentes des 10_markers.
- StratÃ©gie prÃ©position gÃ©nÃ©rique : PREP_GENERIC_CORE (utilisÃ©e pour PREP_SANS_CORE, PREP_MALGRÃ‰_CORE, etc.).
- Contrat LLM renforcÃ© : MUST_COMPLETE_MISSING + rules_obligations (require_subject / require_support / require_cue_completion).
- Sortie enrichie : pipeline_step et placeholder LLM_ajouts.

DÃ©pendances :
    pip install pyyaml regex

EntrÃ©es :
  --rules   : dossier racine contenant 10_markers/ et 20_scopes/
  --input   : fichier texte (1 phrase par ligne)
  --output  : JSONL intermÃ©diaire
  --mode    : strict | permissive  (permissive = MUST_COMPLETE_MISSING)
"""

from __future__ import annotations
import argparse
import json
import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional

import yaml
import regex as reg

# ----------------- logging -----------------

def make_logger(level="INFO"):
    logging.basicConfig(format='[%(levelname)s] %(message)s', level=getattr(logging, level.upper(), logging.INFO))
    return logging.getLogger("runner_v3")

log = make_logger()

# ----------------- tokenisation utils -----------------
PUNCT_RE = r"[,;:!?\.()\[\]{}Â«Â»â€œâ€\"']"
STOPWORDS = {"de","du","des","d'","dâ€™","la","le","les","un","une","et","ou","Ã ","au","aux","en","dans","sur","pour","par","avec","sans","que","ne","nâ€™","n'","pas","plus","jamais","rien","personne","guÃ¨re","point","aucun","aucune"}

DEFAULT_STOP_PUNCT = [",",";",":",".","!","?",")","]","}" ]
# LexÃ¨mes qui coupent naturellement une portÃ©e (concessives/mais)
DEFAULT_STOP_LEXEMES = ["malgrÃ©","mais","cependant","pourtant","toutefois","nÃ©anmoins"]


def tokenize_with_offsets(text: str) -> List[Tuple[str,int,int]]:
    toks: List[Tuple[str,int,int]] = []
    for m in re.finditer(r"\S+", text, flags=re.UNICODE):
        chunk = m.group(0)
        s = m.start()
        last = 0
        for pm in re.finditer(PUNCT_RE, chunk):
            if pm.start() > last:
                toks.append((chunk[last:pm.start()], s+last, s+pm.start()))
            toks.append((pm.group(0), s+pm.start(), s+pm.end()))
            last = pm.end()
        if last < len(chunk):
            toks.append((chunk[last:], s+last, s+len(chunk)))
    return [(t,a,b) for (t,a,b) in toks if t]


def window_right(tokens: List[Tuple[str,int,int]], start_char: int, max_tokens: int,
                 stop_punct: Optional[List[str]] = None,
                 stop_lexemes: Optional[List[str]] = None) -> Tuple[int,int]:
    """Collecte une fenÃªtre Ã  droite jusqu'Ã  ponctuation ou lexÃ¨me coupeur."""
    stop = set(stop_punct or DEFAULT_STOP_PUNCT)
    cutters = {w.lower() for w in (stop_lexemes or DEFAULT_STOP_LEXEMES)}
    collected = []
    for tok,a,b in tokens:
        if a < start_char:
            continue
        if tok in stop:
            break
        if tok.lower() in cutters:
            break
        collected.append((a,b))
        if len(collected) >= max_tokens:
            break
    if not collected:
        return -1,-1
    return collected[0][0], collected[-1][1]


def strip_leading_de(span: str) -> str:
    return re.sub(r"^(?:de|dâ€™|d')\s+", "", span, flags=re.IGNORECASE)


def normalize_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s.strip()).replace("â€™","'")

# ----------------- chargement YAML -----------------

def _iter_yaml_files(folder: Path) -> List[Path]:
    return sorted([p for p in folder.glob("*.yaml") if p.is_file() and not p.name.startswith("_")])


def infer_group_from_filename(name: str) -> str:
    n = name.lower()
    if "ni" in n or "conjonction" in n:
        return "conjonction"
    if "ne_pas" in n or "bipartites" in n or "ne_" in n:
        return "bipartite"
    if "preposition" in n or "sans" in n or "prep" in n:
        return "preposition"
    if "det" in n or "aucun" in n or "determinant" in n or "pas_de" in n:
        return "determinant"
    if "locution" in n:
        return "locution"
    if "lexical" in n:
        return "lexical"
    if "adversative" in n or "mais" in n:
        return "adversative"
    return "autres_marqueurs"


def load_markers(rules_dir: Path) -> Dict[str, List[Dict[str, Any]]]:
    d = rules_dir / "10_markers"
    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for f in _iter_yaml_files(d):
        items = yaml.safe_load(f.read_text(encoding="utf-8"))
        if not isinstance(items, list):
            continue
        for rule in items:
            rule["_file"] = str(f)
            gid = rule.get("group") or infer_group_from_filename(f.name)
            rule["group"] = gid
            pat = rule.get("when_pattern")
            if pat:
                flags = reg.IGNORECASE if rule.get("options",{}).get("case_insensitive") else 0
                rule["_compiled"] = reg.compile(pat, flags)
            comp_guards = []
            for g in rule.get("negative_guards", []) or []:
                gp = g.get("pattern") if isinstance(g, dict) else g
                if gp:
                    comp_guards.append(reg.compile(gp, reg.IGNORECASE))
            if comp_guards:
                rule["_guards"] = comp_guards
            grouped.setdefault(gid, []).append(rule)
    for g,L in grouped.items():
        log.info("Markers '%s': %d rÃ¨gles", g, len(L))
    return grouped


def load_registry_and_scopes(rules_dir: Path) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, List[str]]]:
    dir20 = rules_dir / "20_scopes"
    reg_path = dir20 / "_registry.yaml"
    order_by_group: Dict[str, List[str]] = {}
    if reg_path.exists():
        raw = yaml.safe_load(reg_path.read_text(encoding="utf-8"))
        routes = raw.get("routes_by_group", raw)
        for g, cfg in routes.items():
            if isinstance(cfg, dict) and "strategies" in cfg:
                order_by_group[g] = list(cfg["strategies"])
            elif isinstance(cfg, list):
                order_by_group[g] = list(cfg)
    strategies: Dict[str, Dict[str, Any]] = {}
    for f in _iter_yaml_files(dir20) + [p for p in dir20.glob("*.yaml") if p.is_file() and not p.name.startswith("_")]:
        try:
            items = yaml.safe_load(f.read_text(encoding="utf-8"))
        except Exception:
            continue
        if not isinstance(items, list):
            continue
        for s in items:
            sid = s.get("id");
            if not sid: continue
            s["_file"] = str(f)
            strategies[sid] = s
    log.info("StratÃ©gies chargÃ©es: %d", len(strategies))
    for g, L in order_by_group.items():
        log.info("Ordre '%s': %s", g, ", ".join(L))
    return strategies, order_by_group

# ----------------- dÃ©tection des cues (10_markers) -----------------

def apply_marker_rule(rule: Dict[str,Any], text: str) -> List[Dict[str,Any]]:
    """Apply a single marker rule to the input text and return a list of cue objects.

    The rules defined in ``10_markers`` can be of different natures: some are pure
    detection patterns (which should emit a cue when their ``when_pattern`` matches),
    and some are qualityâ€‘control or guard rules (which are described by the ``action``
    field or have an id starting with ``QC_``). QC rules do not correspond to
    negation markers themselves; instead they instruct the LLM to perform a
    validation or repair on the cues produced by the deterministic layer. If we
    treat them like any other detection rule, they end up generating bogus cues
    that cover the entire sentence or empty spans. To avoid polluting the cue
    output, we simply skip any rule that declares an ``action`` or whose ``id``
    starts with ``QC_``. These rules are forwarded to the LLM via
    ``pending_rules`` but do not create cues here.
    """
    out: List[Dict[str,Any]] = []
    # -------------------------------------------------------------------------
    # Skip QC/validation rules: they shouldn't emit cues. We detect them via
    # the presence of an "action" field or an id starting with "QC_".
    if rule.get("action") or (str(rule.get("id", "")).upper().startswith("QC_")):
        return out

    pat = rule.get("_compiled")
    if not pat and not rule.get("when_pattern"):
        wm = rule.get("when_marker")
        if wm:
            for m in re.finditer(rf"\b{re.escape(wm)}\b", text, flags=re.IGNORECASE):
                if not _guard_hits(rule, text, m):
                    out.append(_mk_cue(rule, m.start(), m.end(), text[m.start():m.end()]))
        return out
    if not pat:
        flags = reg.IGNORECASE if rule.get("options",{}).get("case_insensitive") else 0
        try:
            pat = reg.compile(rule["when_pattern"], flags)
        except Exception:
            return out
    for m in pat.finditer(text):
        if _guard_hits(rule, text, m):
            continue
        label = _format_cue_label(rule.get("cue_label"), m)
        out.append({
            "id": rule.get("id","UNK_RULE"),
            "cue_label": normalize_spaces(label),
            "start": m.start(),
            "end": m.end(),
            "group": rule.get("group","unknown"),
        })
    return out


def _mk_cue(rule: Dict[str,Any], a:int,b:int, span:str) -> Dict[str,Any]:
    return {"id": rule.get("id","UNK_RULE"), "cue_label": span, "start": a, "end": b, "group": rule.get("group","unknown")}


def _guard_hits(rule: Dict[str,Any], text: str, match=None) -> bool:
    guards = rule.get("_guards") or []
    if not guards: return False
    window = text
    if match:
        a = max(0, match.start()-40); b = min(len(text), match.end()+40)
        window = text[a:b]
    return any(g.search(window) for g in guards)


def _format_cue_label(lbl, m) -> str:
    if lbl is None:
        return m.group(0)
    if isinstance(lbl, list):
        for cand in lbl:
            try:
                v = cand.format(**m.groupdict(default="")).strip()
            except Exception:
                v = cand
            if v:
                return v
        return lbl[0]
    if isinstance(lbl, str):
        try:
            return lbl.format(**m.groupdict(default="")).strip()
        except Exception:
            return lbl
    return str(lbl)

# ----------------- complÃ©ments dÃ©terministes (prÃ©positions concessives) -----------------
# Certains corpus n'ont pas de rÃ¨gle 10_markers pour Â« malgrÃ© Â». On l'ajoute de faÃ§on dÃ©terministe si absent.
SURFACE_PREP_MARKERS = {
    "malgrÃ©": {"id": "PREP_MALGRÃ‰", "group": "preposition", "cue_label": "malgrÃ©"},
}

def inject_surface_markers(text: str, cues: List[Dict[str,Any]]) -> List[Dict[str,Any]]:
    tlow = text.lower()
    have = {(c["group"], c["cue_label"].lower()) for c in cues}
    added: List[Dict[str,Any]] = []
    for lex, meta in SURFACE_PREP_MARKERS.items():
        if (meta["group"], lex) in have:
            continue
        for m in re.finditer(rf"\b{re.escape(lex)}\b", tlow):
            a,b = m.start(), m.end()
            added.append({
                "id": meta["id"],
                "group": meta["group"],
                "cue_label": meta["cue_label"],
                "start": a,
                "end": b,
            })
    return cues + added

# ----------------- guards de stratÃ©gie (20_scopes) -----------------

def guard_deny_if_surface(strategy: Dict[str,Any], text: str) -> bool:
    cfg = (strategy.get("guards") or {}).get("deny_if_surface")
    if not cfg: return False
    for g in cfg:
        pat = g.get("pattern") if isinstance(g, dict) else g
        if not pat: continue
        flags = reg.IGNORECASE if (isinstance(g, dict) and ((g.get("options") or {}).get("case_insensitive"))) else 0
        rx = reg.compile(pat, flags)
        if rx.search(text):
            return True
    return False

# ----------------- exÃ©cution des stratÃ©gies (sans spaCy) -----------------

def exec_strategy(group: str, sid: str, strat: Dict[str,Any], text: str, tokens,
                  cues_for_group: List[Dict[str,Any]], all_group_scopes: Dict[str,List[Dict[str,Any]]]) -> List[Dict[str,Any]]:
    stype = strat.get("scope_strategy") or ""
    opts = strat.get("options") or {}

    # deny_if_surface
    if guard_deny_if_surface(strat, text):
        return []

    # === SKIP_IF_PATTERN ===
    if stype == "SKIP_IF_PATTERN":
        pat = (opts or {}).get("pattern")
        if pat and reg.search(pat, text):
            return [{"_skip_group": group, "strategy": sid}]
        return []

    # === SKIP_IF_LEXICALIZED (prÃ©positions lexicalisÃ©es) ===
    if stype == "SKIP_IF_LEXICALIZED":
        patterns = (opts or {}).get("lexicalized_patterns", [])
        for p in patterns:
            if re.search(rf"\b{re.escape(p)}\b", text, flags=re.IGNORECASE):
                return [{"_skip_group": group, "strategy": sid}]
        return []

    # === NEP_SMART (bipartite core) ===
    if stype == "NEP_SMART" or (sid.endswith("_CORE") and group=="bipartite"):
        out = []
        for c in cues_for_group:
            out.append({
                "id": sid if sid else "BIP_G_CORE",
                "scope": "<LLM_APPLY_RULE>",   # placeholder, scope final attendu du LLM
                "start": c["end"],
                "end": -1,
                "llm_instruction": {
                    "policy": "READ_AND_APPLY_RULE",
                    "rule_id": sid,
                    "rule_yaml": strat,   # dump complet de la rÃ¨gle YAML
                    "cue": c,
                    "text": text,
                    "notes": "Relis la rÃ¨gle YAML, Ã©cris la rÃ¨gle, applique-la au cue pour gÃ©nÃ©rer le scope correct. Respecte start/end."
                }
            })
        return out


    # === DET_NEG_GN_SMART (determinant core) ===
    if stype == "DET_NEG_GN_SMART" or (sid.endswith("_CORE") and group=="determinant"):
        out=[]
        max_tokens = int(opts.get("max_token_gap", 8))
        stop_punct = opts.get("stop_punct") or DEFAULT_STOP_PUNCT
        stop_lexemes = opts.get("stop_lexemes") or DEFAULT_STOP_LEXEMES
        strip_de = opts.get("strip_de", True)
        for c in cues_for_group:
            a,b = window_right(tokens, c["end"], max_tokens, stop_punct, stop_lexemes)
            if a==-1: continue
            span = normalize_spaces(text[a:b])
            if strip_de: span = strip_leading_de(span)
            sa = text.lower().find(span.lower(), c["end"]) ; sb = sa + len(span) if sa!=-1 else -1
            out.append({"id": sid if sid else "DET_G_CORE", "scope": span, "start": sa, "end": sb})
        return dedup(out)

    # === PREP cores (gÃ©nÃ©rique) ===
    if stype in {"PREP_SANS_CORE","PREP_MALGRÃ‰_CORE","PREP_GENERIC_CORE"} or (sid.endswith("_CORE") and group=="preposition"):
        out=[]
        max_tokens = int(opts.get("right_window_tokens", 10))
        stop_punct = opts.get("stop_punct") or DEFAULT_STOP_PUNCT
        stop_lexemes = opts.get("stop_lexemes") or DEFAULT_STOP_LEXEMES
        for c in cues_for_group:
            a,b = window_right(tokens, c["end"], max_tokens, stop_punct, stop_lexemes)
            if a==-1: continue
            span = strip_leading_de(normalize_spaces(text[a:b]))
            out.append({"id": sid or "PREP_GENERIC_CORE", "scope": span, "start": a, "end": b})
        return dedup(out)

    # === NI coord ===
    if stype in {"NI_COORD_SMART","NI_SIMPLE_SPLIT"} or (sid.endswith("_CORE") and group=="conjonction"):
        out=[]
        tlow = text.lower()
        parts = re.split(r"\bni\b", tlow)
        for part in parts[1:]:
            frag = part.strip()
            if not frag: continue
            frag = re.split(PUNCT_RE, frag)[0].strip()
            if not frag: continue
            a = tlow.find(frag)
            if a!=-1:
                b = a+len(frag)
                out.append({"id": sid if sid else "CONJ_NI_CORE", "scope": text[a:b], "start": a, "end": b})
        return dedup(out)

    # === COOC_DET_RESOLVE (bipartite <-> determinant) ===
    if stype == "RESOLVE_COOCURRENCE" or sid.endswith("COOC_DET_RESOLVE"):
        out=[]
        bip = all_group_scopes.get("bipartite", [])
        det = all_group_scopes.get("determinant", [])
        for b1 in bip:
            for d1 in det:
                if spans_overlap(b1, d1) >= 0.5:
                    a = min(b1["start"], d1["start"]) ; c = max(b1["end"], d1["end"])
                    out.append({"id": sid, "scope": normalize_spaces(text[a:c]), "start": a, "end": c})
        return dedup(out)

    # === GOVERNOR_SUPPORT_AUTO (supports surface) ===
    if stype == "GOVERNOR_SUPPORT_AUTO":
        out=[]
        for pre in ["selon","d'aprÃ¨s","dâ€™\u2009aprÃ¨s","conformÃ©ment Ã ","au regard de","en accord avec","par"]:
            m_iter = re.finditer(rf"\b{re.escape(pre)}\b\s+([^.,;:]+)", text, flags=re.IGNORECASE)
            for m in m_iter:
                a = m.start(1); b = m.end(1)
                out.append({"id": sid, "role": "support", "scope": normalize_spaces(text[a:b]), "start": a, "end": b})
        # Titre Ã  gauche avant ':'
        m = re.search(r"(^|[\n\.])([^\n:\.]{3,})\s*:[\s\-â€“â€”]*$", text)
        if m:
            a = m.start(2); b = m.end(2)
            out.append({"id": sid, "role": "support", "scope": normalize_spaces(text[a:b]), "start": a, "end": b})
        return dedup(out)

    # Fallback: when_pattern au niveau stratÃ©gie
    wpat = strat.get("when_pattern")
    if wpat:
        flags = reg.IGNORECASE if (strat.get("options") or {}).get("case_insensitive") else 0
        rx = reg.compile(wpat, flags)
        out=[]
        for m in rx.finditer(text):
            a,b = m.start(), m.end()
            out.append({"id": sid, "scope": normalize_spaces(text[a:b]), "start": a, "end": b})
        return dedup(out)

    return []

# ----------------- chevauchements & QC -----------------

def spans_overlap(x, y) -> float:
    a1,a2 = x["start"], x["end"]; b1,b2 = y["start"], y["end"]
    inter = max(0, min(a2,b2) - max(a1,b1))
    den = max(a2,b2) - min(a1,b1)
    return inter/den if den>0 else 0.0


def qc_trim_appositions(text: str, span: Dict[str,Any]) -> Dict[str,Any]:
    tail = text[span["start"]:span["end"]]
    for t in [", selon", ", d'aprÃ¨s", ", dâ€™\u2009aprÃ¨s", ", conformÃ©ment Ã ", ", au regard de", ", en accord avec"]:
        idx = tail.lower().find(t)
        if idx != -1:
            span["end"] = span["start"] + idx
            span["scope"] = normalize_spaces(text[span["start"]:span["end"]])
            break
    return span


def apply_qc(scopes: List[Dict[str,Any]], text: str) -> List[Dict[str,Any]]:
    out=[]
    for s in scopes:
        if s.get("role") == "support":
            out.append(s); continue
        if s.get("start",-1)>=0 and s.get("end",-1)>=0:
            s = qc_trim_appositions(text, s)
        out.append(s)
    return dedup(out)


def dedup(spans: List[Dict[str,Any]]) -> List[Dict[str,Any]]:
    seen=set(); out=[]
    for s in spans:
        key=(s.get("id"), s.get("start"), s.get("end"), s.get("role"))
        if key in seen: continue
        seen.add(key); out.append(s)
    return out

# -----------------------------------------------------------------------------
# DÃ©tection bipartite crossâ€‘tokens
# -----------------------------------------------------------------------------

def detect_bipartite_cross_tokens(text: str, tokens: List[Tuple[str,int,int]], existing_cues: List[Dict[str,Any]], rule: Dict[str,Any]) -> List[Dict[str,Any]]:
    """
    DÃ©tecte les marqueurs bipartites de type Â«â€¯ne â€¦ pasâ€¯Â» lorsque les deux
    Ã©lÃ©ments sont sÃ©parÃ©s par un ou plusieurs tokens. La logique est guidÃ©e par
    les options ``max_token_gap`` dÃ©finies dans la rÃ¨gle YAML. Les listes des
    particules de dÃ©part (part1) et de fin (part2) sont dÃ©rivÃ©es du YAML
    (voir ``bipartites.yaml``) pour NE_BIPARTITE_EXTENDED.

    :param text: la phrase brute
    :param tokens: liste des tokens avec offsets
    :param existing_cues: cues dÃ©jÃ  dÃ©tectÃ©s (pour Ã©viter les doublons)
    :param rule: la rÃ¨gle YAML correspondante (doit contenir options.max_token_gap)
    :return: liste de nouvelles cues Ã  ajouter
    """
    out = []
    # Only apply if the rule declares a max_token_gap; otherwise the standard regex suffira.
    max_tokens = rule.get("options", {}).get("max_token_gap")
    if not max_tokens:
        return out
    try:
        max_tokens = int(max_tokens)
    except Exception:
        max_tokens = 8
    # Hardâ€‘code the list of possible second parts; these mirror the YAML definition.
    part1_set = {"ne", "n'", "nâ€™"}
    part2_set = {"pas", "plus", "jamais", "rien", "personne", "guÃ¨re", "point", "nul"}
    existing_keys = {(c.get("id"), c.get("start"), c.get("end")) for c in existing_cues}
    for i, (tok, a, b) in enumerate(tokens):
        t = tok.lower()
        # un marqueur de dÃ©but peut Ãªtre "ne", ou bien un token dÃ©butant par "n'" ou "nâ€™"
        is_part1 = t == "ne" or t.startswith("n'") or t.startswith("nâ€™")
        if is_part1:
            # search ahead within max_tokens tokens (excluding punctuation tokens)
            gap = 0
            for j in range(i+1, len(tokens)):
                t2, a2, b2 = tokens[j]
                # ignore pure punctuation tokens (e.g., commas). We consider them as gap but don't count them.
                if re.match(PUNCT_RE, t2):
                    continue
                gap += 1
                if gap > max_tokens:
                    break
                if t2.lower() in part2_set:
                    # ensure we don't already have this cue
                    key = (rule.get("id"), a, b2)
                    if key not in existing_keys:
                        label = normalize_spaces(tok + " " + t2)
                        out.append({
                            "id": rule.get("id", "NE_BIPARTITE_EXTENDED"),
                            "cue_label": label,
                            "start": a,
                            "end": b2,
                            "group": rule.get("group", "bipartite")
                        })
                    break
    return out

# ----------------- contrat LLM -----------------
NEG_LEXEMES_HINTS = {
    "bipartite": ["ne","nâ€™","n'","pas","plus","jamais","rien","personne","guÃ¨re","point","nul"],
    "determinant": ["aucun","aucune","aucuns","aucunes","pas de","pas dâ€™","pas d'"],
    "preposition": ["sans","hors de","en dehors de","malgrÃ©"],
    "conjonction": ["ni"],
    "lexical": ["non","inefficace","impossible","absent","absence","nul","nulle"],
    "locution": ["Ã  lâ€™exception de","Ã  l'exclusion de","privation de","faute de"],
    "adversative": ["mais"],
}


def make_llm_contract(mode: str, text: str, cues: List[Dict[str,Any]]) -> Dict[str,Any]:
    if mode=="strict":
        return {"mode":"strict"}
    found_texts = {c["cue_label"].lower() for c in cues}
    tlow = text.lower()
    hints=[]
    missing_groups=[]
    for g, lex in NEG_LEXEMES_HINTS.items():
        present=False
        for w in lex:
            if w.lower() in tlow:
                present=True
                if w.lower() not in found_texts:
                    hints.append({"group": g, "lexeme": w})
        if present and g not in {c["group"] for c in cues}:
            missing_groups.append(g)
    policy=(
        "MUST_COMPLETE_MISSING: Tu DOIS complÃ©ter tout sujet, support ET tout marqueur manquant "
        "en respectant les rÃ¨gles YAML. "
        "ExÃ©cute aussi toutes les pending_rules fournies : relis la rÃ¨gle, Ã©cris la rÃ¨gle et applique-la au cue."
        
        "MUST_COMPLETE_MISSING: Tu DOIS complÃ©ter tout sujet, support ET tout marqueur manquant en respectant les rÃ¨gles YAML. "
        "PrioritÃ© absolue aux candidats du runner. Si un lexÃ¨me/structure indique une nÃ©gation non couverte, "
        "ajoute-la avec id='LLM_FALLBACK' et group='inference', puis calcule la portÃ©e avec les mÃªmes stratÃ©gies. "
        "Fusionne les scopes identiques. Respecte strictement le schÃ©ma de sortie."
    )
    rules_obligations = [
        {"id":"BIP_G_CORE","require_subject": True, "description":"Inclure le sujet grammatical dans une portÃ©e distincte."},
        {"id":"GOVERNOR_SUPPORT_AUTO","require_support": True, "description":"Inclure les entitÃ©s de support normatives (selon, dâ€™aprÃ¨s, etc.)."},
        {"id":"ALL_MARKERS","require_cue_completion": True, "description":"ComplÃ©ter les marqueurs manquants si un lexÃ¨me connu apparaÃ®t, mÃªme si redondant."},
    ]
    fallback_schema={
        "cue": {"id":"LLM_FALLBACK","group":"inference","cue_label":"<forme canonique>","start":-1,"end":-1},
        "scope": {"id":"<STRATEGY_ID>","scope":"<texte>","start":-1,"end":-1}
    }
    return {
        "mode":"permissive_mandatory_completion",
        "policy":policy,
        "rules_obligations": rules_obligations,
        "hints":{"lexeme_hits":hints,"missing_groups":missing_groups},
        "fallback_schema":fallback_schema
    }

# ----------------- builders -----------------

def build_candidates_rules_id_cues(groups_present: List[str], markers_by_group: Dict[str,List[Dict[str,Any]]]) -> List[Dict[str,Any]]:
    out=[]
    for g in groups_present:
        out.append({"group": g, "rules": [r.get("id","UNK_RULE") for r in markers_by_group.get(g,[])]})
    return out


def build_candidates_rules_id_scopes(groups_present: List[str], order_by_group: Dict[str,List[str]]) -> List[Dict[str,Any]]:
    out=[]
    for g in groups_present:
        out.append({"group": g, "rules": list(order_by_group.get(g, []))})
    return out

# ----------------- annotation d'une phrase -----------------

def annotate_sentence(text: str, sid: int, markers_by_group, strategies_by_id, order_by_group, mode: str) -> Dict[str,Any]:
    tokens = tokenize_with_offsets(text)

    # 1) cues dÃ©terministes (10_markers)
    cues: List[Dict[str,Any]] = []
    for g, rules in markers_by_group.items():
        for r in rules:
            cues.extend(apply_marker_rule(r, text))

    # DÃ©tection additionnelle pour les marqueurs bipartites avec un Ã©cart de tokens (ne â€¦ pas, ne â€¦ plus, etc.).
    # Certaines expressions comme Â«â€¯n'ont pasâ€¯Â» ou Â«â€¯ne â€¦ jamaisâ€¯Â» contiennent un ou plusieurs
    # tokens intermÃ©diaires entre les deux parties du marqueur. Les motifs regex prÃ©sents
    # dans les YAML ne capturent que les cas contigus (Â«â€¯ne pasâ€¯Â») et ne tiennent pas compte
    # des options Â«Â max_token_gapÂ Â» dÃ©finies dans les rÃ¨gles. Ici, nous reconstruisons ces
    # marqueurs Ã  partir des tokens lorsque la rÃ¨gle NE_BIPARTITE_EXTENDED est dÃ©finie.
    if "bipartite" in markers_by_group:
        for r in markers_by_group["bipartite"]:
            if r.get("id") == "NE_BIPARTITE_EXTENDED":
                cues.extend(detect_bipartite_cross_tokens(text, tokens, cues, r))
                break
    # ComplÃ©ter certains marqueurs surface (ex. Â«Â malgrÃ©Â Â») s'ils manquent
    cues = inject_surface_markers(text, cues)
    # DÃ©duplication des cues : plusieurs rÃ¨gles peuvent dÃ©tecter le mÃªme marqueur
    # (ex. la prÃ©position Â«â€¯sansâ€¯Â» capturÃ©e par preposition.yaml et locutions.yaml).
    # Afin de respecter le format Â«â€¯un cue = un seul id et un seul groupâ€¯Â», nous
    # Ã©liminons les doublons exacts sur (id, group, cue_label, start, end). Cela
    # conserve un seul objet par occurrence et Ã©vite les rÃ©pÃ©titions dans la sortie.
    unique_cues: Dict[tuple, Dict[str,Any]] = {}
    for c in cues:
        key = (c.get("id"), c.get("group"), c.get("cue_label"), c.get("start"), c.get("end"))
        if key not in unique_cues:
            unique_cues[key] = c
    cues = list(unique_cues.values())

    groups_present = sorted({c["group"] for c in cues})

    # 2) scopes par groupe (ordre registry)
    group_scopes: Dict[str,List[Dict[str,Any]]] = {g: [] for g in groups_present}
    for g in groups_present:
        strats = order_by_group.get(g, [])
        for sid_ in strats:
            strat = strategies_by_id.get(sid_)
            if not strat: continue
            if strat.get("when_group") and strat.get("when_group") != g:
                continue
            res = exec_strategy(g, sid_, strat, text, tokens, [c for c in cues if c["group"]==g], group_scopes)
            if res and any("_skip_group" in r for r in res):
                group_scopes[g] = []
                break
            group_scopes[g].extend(res)

    # 3) QC
    all_scopes = [s for L in group_scopes.values() for s in L]
    all_scopes = apply_qc(all_scopes, text)

    # âž• Nouveau : collecter les instructions LLM
    pending = []
    for s in all_scopes:
        if "llm_instruction" in s:
            pending.append(s["llm_instruction"])
            del s["llm_instruction"]   # on ne garde pas dans scopes

    # 4) objet final (Ã‰tape 1)
    obj = {
        "id": sid,
        "text": text,
        "pipeline_step": "STEP1_DETERMINISTIC",
        "candidates_rules_ID_cues": build_candidates_rules_id_cues(groups_present, markers_by_group),
        "cues": cues,
        "candidates_rules_ID_scopes": build_candidates_rules_id_scopes(groups_present, order_by_group),
        "scopes": [
            {"id": s.get("id","CORE"), "scope": s.get("scope",""),
             "start": s.get("start",-1), "end": s.get("end",-1),
             **({"role": s["role"]} if "role" in s else {})}
            for s in all_scopes
        ],
        # ðŸ”¥ Ici on fusionne avec make_llm_contract
        "llm_contract": {
            **make_llm_contract(mode, text, cues),
            "pending_rules": pending
        },
        "LLM_actions": [],
        "LLM_ajouts": {"cues": [], "scopes": []}
    }
    return obj

# ----------------- CLI -----------------

def main():
    ap = argparse.ArgumentParser(description="Runner permissif v3 (no-NLP, pipeline renforcÃ©)")
    ap.add_argument("--rules", required=True, help="Chemin dossier rules/")
    ap.add_argument("--input", required=True, help="Fichier texte (1 phrase/ligne)")
    ap.add_argument("--output", required=True, help="Sortie JSONL")
    ap.add_argument("--mode", default="permissive", choices=["permissive","strict"], help="Mode LLM")
    ap.add_argument("--log", default="INFO")
    args = ap.parse_args()

    global log; log = make_logger(args.log)

    rules_dir = Path(args.rules)
    markers_by_group = load_markers(rules_dir)
    strategies_by_id, order_by_group = load_registry_and_scopes(rules_dir)

    sid=0
    with open(args.input, "r", encoding="utf-8") as fin, open(args.output, "w", encoding="utf-8") as fout:
        for line in fin:
            text = line.strip()
            if not text: continue
            sid += 1
            obj = annotate_sentence(text, sid, markers_by_group, strategies_by_id, order_by_group, args.mode)
            fout.write(json.dumps(obj, ensure_ascii=False) + "\n")
    log.info("TerminÃ©: %d phrases â†’ %s", sid, args.output)

if __name__ == "__main__":
    main()
